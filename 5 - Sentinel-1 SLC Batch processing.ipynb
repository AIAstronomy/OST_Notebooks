{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"2\"> <i>Open SAR Toolkit - Tutorial 5, version 1.0, September 2019. Andreas Vollrath, ESA/ESRIN phi-lab</i>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](auxiliary/header_image.PNG)\n",
    "\n",
    "--------\n",
    "\n",
    "# OST Tutorial V\n",
    "## How to do a batch processing of Sentinel-1 SLC data based on bursts\n",
    "\n",
    "--------\n",
    "\n",
    "**Short description**\n",
    "\n",
    "This notebook introduces you to the general workflow of OST for the batch processing of SLC data using the *Sentunel1_SCLBatch* class. This is a subclass of the *Sentinel1* class, and thus inherits all the functionality of the *Generic* and the *Sentinel1* classes. In addition, calibrated backscatter will be jointly produced together with the interferometric coherence and the dual-pol H/A/alpha decomposition.\n",
    "\n",
    "--------\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "- a PC/Mac with at least 16GB of RAM\n",
    "- about 50GB of free disk space\n",
    "- a Copernicus Open Data Hub user account, valid for at least 7 days (https://scihub.copernicus.eu)\n",
    "--------\n",
    "\n",
    "**NOTE:** all cells that have an * after its number can be executed without changing any code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1* - Import of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the s1Project class, that basically handles all the workflow from beginning to the end\n",
    "from ost import Sentinel1_SLCBatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Define your project parameters and intialize the class\n",
    "\n",
    "\n",
    "As a subclass of the *Sentinel-1* class, the *Sentinel1_SLCBatch* class inherit all attributes nd methods from the *Generic* and *Sentinel1* class. \n",
    "In addition it provides attributes and methods for the ARD definition, the inventory of bursts, as well as the processing of the bursts to ARD, and the subsequent creation of timeseries, timescans and large-scale mosaics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get home folder\n",
    "import os\n",
    "from os.path import join\n",
    "home = os.getenv('HOME')\n",
    "# create a processing directory\n",
    "project_dir = join('{}'.format(home), 'OpenSarToolkit', 'Tutorial_5')\n",
    "\n",
    "# define a custom temp directory and use an eodata storage on CREODIAS (only applies if you have both set up)\n",
    "#temp_dir = '/ram'\n",
    "#dias_data='/eodata'\n",
    "\n",
    "# apply function with buffer in meters\n",
    "from ost.helpers import vector\n",
    "lat, lon = '50.68', '10.92'\n",
    "aoi = vector.latlon_to_wkt(lat, lon, buffer_meter=6000, envelope=True)\n",
    "#----------------------------\n",
    "# Time of interest\n",
    "#----------------------------\n",
    "# we set only the start date to today - 30 days\n",
    "start = '2018-01-01'\n",
    "end = '2018-01-20'\n",
    "\n",
    "# create s1Project class instance\n",
    "s1_batch = Sentinel1_SLCBatch(\n",
    "    project_dir=project_dir, \n",
    "    aoi=aoi, \n",
    "    start=start, \n",
    "    end=end, \n",
    "    product_type='SLC', \n",
    "    #temp_dir=temp_dir,\n",
    "    #data_mount=dias_data, \n",
    "    ard_type='OST Plus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3* - Data inventory\n",
    "\n",
    "We take the known functionality from the *Sentinel1* class to search for available acquisitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------\n",
    "# for plotting purposes we use this iPython magic\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (19, 19)\n",
    "#---------------------------------------------------\n",
    "\n",
    "# search command\n",
    "s1_batch.search()\n",
    "# we plot the full Inventory on a map\n",
    "s1_batch.plot_inventory(transperancy=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_batch.refine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4* - Select a mosaic key\n",
    "\n",
    "We select one of our mosaic key, seen in the summarising information of the previous refine command, and check the coverage by plotting the footprints on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = (13, 13)\n",
    "\n",
    "\n",
    "key = 'DESCENDING_VVVH'\n",
    "s1_batch.refined_inventory_dict[key]\n",
    "s1_batch.plot_inventory(s1_batch.refined_inventory_dict[key], 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5*) - Download of scenes\n",
    "\n",
    "The download routine, already known from the *Sentinel1* class, is used to download the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_batch.download(s1_batch.refined_inventory_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Creating a burst inventory\n",
    "\n",
    "Sentinel-1 does not come with predefined footprints. This is a problem when working on larger areas and coherence should be processed, which results from two images of the same orbit taken at different times. However, there is a smaller part of the imagery referred to as bursts, which is uniquelly defined on its position by the AnxTime attribute in the Sentinel-1 annotation files. By creating a database, that holds a burst id, consisting of orbit, subswath and AnxTime, and the repsective scene and its burst number, we can subsequently process burst by burst. \n",
    "\n",
    "The advantages are:\n",
    "- less memory needs\n",
    "- can be more easily scaled for parallel processing on clusters\n",
    "- processing is reduced by considering only the bursts actually overlapping with the AOI\n",
    "- by using SLC data, polarimetric decompositions as well as interferometric coherence can be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_batch.create_burst_inventory(key=key, refine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = (13, 13)\n",
    "s1_batch.plot_inventory(s1_batch.burst_inventory, transperancy=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Our burst inventory holds {} bursts to process.'.format(len(s1_batch.burst_inventory)))\n",
    "print('------------------------------------------')\n",
    "print(s1_batch.burst_inventory.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - Analysis Ready-Data format for various SLC products and the multi-temporal data products\n",
    "\n",
    "ARD formats for SLC data, apart from the backscatter are not well defined. Nonetheless, OST allows to produce the dual-pol H/A/Alpha decomposition as well as the interferometric coherence of consecutive acquisitions in an automated manner. \n",
    "\n",
    "In addition, the time-series of the resulting products can be produced, first burst-by-burst, and then followed by mosaicking. The same is true for the generation of timescan, where 5 different statistics can be computed.\n",
    "\n",
    "For the creation of time-series 2 additional parameters are added:\n",
    "- *to_db_mt*: output the time-series data in dB scale\n",
    "- *mt_speckle-filter*: apply the multi-temporal speckle filter (standard Snap parameters, i.e. Refined Lee)\n",
    "\n",
    "For the creation of the timescans 2 additional parameters are added:\n",
    "- *metrics*: The stastical measures applied over each timeseries. By default all measures, consisting of average (avg), maximum (max), minimum (min), standard deviation (std) and Coeffcient of Variance (cov)\n",
    "- *outlier_removal*: In some cases images are affected by artifacts with strong signals. For the calculation of the timescan layers, extreme values can be excluded setting this parameter to True.\n",
    "\n",
    "\n",
    "For this example exercise we customize some of the ARD parameters to create aminimal processing example and save on prcessing time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------------------------')\n",
    "print(' Original OST Plus ARD parameters')\n",
    "print(s1_batch.ard_parameters)\n",
    "print('---------------------------------')\n",
    "\n",
    "s1_batch.ard_parameters['product_type'] = 'GTCgamma'\n",
    "s1_batch.ard_parameters['ls_mask_create'] = False\n",
    "s1_batch.ard_parameters['ls_mask_apply'] = False\n",
    "s1_batch.ard_parameters['to_db'] = False\n",
    "s1_batch.ard_parameters['speckle_filter'] = False\n",
    "s1_batch.ard_parameters['pol_speckle_filter'] = False\n",
    "s1_batch.ard_parameters['mt_speckle_filter'] = False\n",
    "s1_batch.ard_parameters['to_db_mt'] = True\n",
    "s1_batch.ard_parameters['resolution'] = 30\n",
    "\n",
    "print('---------------------------------')\n",
    "print(' Customized ARD parameters')\n",
    "print(s1_batch.ard_parameters)\n",
    "print('---------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8* - Launch processing\n",
    "\n",
    "The *burst_to_ard* method allows to execute the full workflow consiting of:\n",
    "1 Burst to ARD: each burst undergoes the ARD generation routine.\n",
    "2 ARD to Timeseries: a timeseries is created for all overlapping bursts\n",
    "3 Timeseries to Timescan: the multi-temporal metrics are calculated for all burst timeseries\n",
    "4 Mossaicking: All bursts of the timeseries as well as the timescans are mosaicked to a single file\n",
    "\n",
    "**Note** The overwrite parameter allows to start from scratch, meaning that all previously processed data is deleted. Otherwise the routine will restart from where it stopped. At the moment this aplies only for the Burst to ARD routine, which is the most processing intense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_batch.burst_to_ard(timeseries=True, timescan=True, mosaic=True, overwrite=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
